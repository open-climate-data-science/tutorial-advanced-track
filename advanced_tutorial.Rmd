---
title: "North Carolina State Climate Office "
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#package for plotting 
library(ggplot2)
#package for cleaning data and logical things 
library(tidyverse)
# weather station data from NOAA API 
library(rnoaa)

#time series related 
library(forecast)
#install.packages("xts")                      # Install & load xts package
library("xts")

library(ClusterR)
library(cluster)
library(broom)

#other packages for kmeans
library(here)
library(tidymodels)
```

# OCDS Advanced Track Tutorial{.tabset}


## Tutorial Learning Goals

Welcome to the NCSCO OCDS Intro track tutorial. We are  [Nick Gawron](https://www.linkedin.com/in/ngawrondata/) and [Livia Popa](https://www.linkedin.com/in/livia-popa-23a018183/), we will be working with you through today's tutorial.  We will be using R studio for today's session. 

We will be tackling these objectives:

- *Define* open data and reproducible science
- *Describe* how to navigate important aspects of the R/RStudio user-interface
- *Recall* how to extract public data from a web portal (Cardinal) and import it into a  data software (R/RStudio)
- *Demonstrate* understanding of dataset and statistical software through exploratory data analysis plots and numerical summaries

### Meet Mr. Wuf!

Mr. Wuf works for Mount Mitchell State Park in Burnsville, NC and was recently asked by his boss to write a report summarizing rainfall and temperature data for 2021. This report will be used to help optimize 2022 event planning (e.g., fall color viewing) for park visitors and maintenance scheduling for park staff. Mr. Wuf’s wife, Mrs. Wuf, recently told him about the State Climate Office of North Carolina’s new Cardinal and Station Scout data portals. He agrees with her that it would be a great opportunity to check out these new, free tools. After some preliminary sleuthing around Station Scout, he discovered there was a National Weather Service Cooperative Observer Program (COOP; https://www.weather.gov/rah/coop) station on park property (station # 315923). How did he miss this? Once he downloads these data from Cardinal, Mr. Wuf plans to put the skills he learned in an online R programming course to the test for this real-world, work-related project.\

## API 

### API Introduction 
- API is the acronym for Application Programming Interface that enables applications to exchange data and functionality easily and securely.

- The API data that we will be using is called nClimgrid which consists of four climate variables derived from the GHCN-Dataset: maximum temperature, minimum temperature, average temperature, and precipitation. 

### nClimgrid Data Set

```{r}

## Read in data from AWS nClimGrid
nclim<-read_csv('https://noaa-nclimgrid-daily-pds.s3.amazonaws.com/EpiNOAA/decadal/2020-2021-ste-decadal.csv')%>%drop_na()

```

```{r}
## inspect data 

view(nclim)
ggpolt(nclim,aes(x=date,y=TMIN))+geom_point(colour="green")
```

## EDA
- We will inspect the data through a couple graphs as well as a simple linear regression

```{r}
# scatterplot of 
ggplot(nclim,aes(x=date,y=PRCP))+geom_point()
```



## Machine Learning 

### K-Means Clustering in nClimGridData

#### Creating Functions

```{r}
calculate_cluster <- function(data, k) {
  x <- data %>%
    na.omit() %>%
    scale()
  
  df <- kmeans(x, center = k) %>%
    augment(data) %>% # creates column ".cluster" with cluster label
    mutate(silhouette = cluster::silhouette(as.integer(.cluster), dist(x))[, "sil_width"]) # calculate silhouette score
  
  return(df)
}

```


```{r}
plot(nclim$PRCP,nclim$TAVG)

data2cluster<-data.frame(nclim$PRCP, nclim$TAVG)

df<-calculate_cluster(data2cluster,4)
df
```






### PCA from Econet Data 

```{r}

cardinal <- read_csv("cardinal_data.csv", 
     col_types = list(`Average Air Temperature (F)` = col_number(), 
         `Maximum Air Temperature (F)` = col_number(), 
         `Minimum Air Temperature (F)` = col_number(), 
         `Average Experimental Leaf Wetness (mV)` = col_number(), 
         `Total Precipitation (in)` = col_number(), 
         `Average Relative Humidity (%)` = col_number(), 
         `Average Soil Moisture (m3/m3)` = col_number(), 
         `Average Soil Temperature (F)` = col_number(), 
         `Average Solar Radiation (W/m2)` = col_number(), 
         `Average Station Pressure (mb)` = col_number()))

cardinal<-drop_na(cardinal)
str(cardinal)
cardinal$Date<-as.Date(cardinal$Date, tryFormats= c("%m/%d/%y"))
view(cardinal)

#changes col names
colnames(cardinal)=c("date","AvgT","MaxT","MinT","AvgLw","Tprep","AvgHum","AvgSm","AvgSt","AvgSr","AvgStp")



cardinal$IfRain<- (cardinal$Tprep>0)
cardinal$IfRain<-as.factor(as.integer(cardinal$IfRain))


```

### Basic Plotting with Ggplot 


```{r}
ggplot(cardinal,aes(x=date,y=AvgT))+geom_line()+labs(title="Total Daily Rainfall by Date",y="Average Tempurature (F) ", x= "Date")
```

- EDA is how we can motivate future ML models!

- We can use forecasting to extend this trend!


## Testing and training data

- How can we see how well a model works? 

![Cross Validation Concept](images/CrossValidation.png)

### Time Series forecasting


- We were thinking of using logistic regression  but may not?

```{r}
#logistic regression
fit1 = glm(IfRain~date+AvgT+AvgLw+AvgSt+AvgSr, data=cardinal, family="binomial")
summary(fit1)

#predict something with logistic regression

```




```{r}

# n climate  grid data

temp_ts <- xts(cardinal$AvgT,cardinal$date)
head(temp_ts)
  
autoplot(temp_ts[1:600])

half_temp <-temp_ts[1:600]

library(forecast)
d.arima <- auto.arima(half_temp)
d.forecast <- forecast(d.arima, level = c(90), h = 200)
autoplot(d.forecast)

```


### PCA to cluster rain variable 


- using cardinal data to obsevre *if* there is clustering 

- used for future models

- helps us describe higher dimensional data with **less**


Three general steps: 

  1. Remove heavily correlated columns! 
    - Min Temp and Max Temp for a certain day will correlate with one another!
    
  2. Center Data

Observe: 

```{r}
library(corrplot)
corrplot(cor(cardinal[,-c(1,12)]))
```

- Tells us to remove all but one temperature variable


```{r}
IfRainVar<- cardinal$IfRain
cardshort <- cardinal%>%select(-c(date,IfRain,Tprep,MinT,MaxT))
cardshort

pca_card<- princomp(scale(cardshort,scale=FALSE),cor = FALSE)


plot(pca_card$scores, pch = 16, col =IfRainVar)
legend("topright",c("No Rain","Rain"),pch=16,col=c("black","red"))

```

- Here we can look at how good PCA does at describing changes in data 

- We see 2 components describes 96% of the data's variation ! (This is very good)

```{r}
summary(pca_card)
```



```{r}
screeplot(pca_card, type = "lines")
```


### How are the original variables related to the principal components?

- Does not print small values, less impactful to correlation 

```{r}
loadings(pca_card)
```


- The loading are simple correlations between the principal components and the original variables (Pearson’s r).

- Values closest to 1 (positive) or -1 (negative) will represent the strongest relationships, with zero being uncorrelated.

We see in PC 1 that there is a high positive correlation between AvgSr. We see the correlation between solar radiation and the component direction is quite high. So by looking at the second component or the y-axis of our previous plot: we see for the most part, Leaf wetness correlated well with the occurance of rain.   


- Another visual to observe the impact of each variable on the principal component!  

- Not super pretty here

```{r}
biplot(pca_card)
```





