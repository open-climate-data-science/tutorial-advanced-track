---
title: "North Carolina State Climate Office "
output:
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#package for plotting 
library(ggplot2)
#package for cleaning data and logical things 
library(tidyverse)
# weather station data from NOAA API 
library(rnoaa)

#time series related 
library(forecast)
#install.packages("xts")                      # Install & load xts package
library("xts")

library(ClusterR)
library(cluster)
```

# OCDS Advanced Track Tutorial{.tabset}


## Tutorial Learning Goals

Welcome to the NCSCO OCDS Intro track tutorial. We are  [Nick Gawron](https://www.linkedin.com/in/ngawrondata/) and [Livia Popa](https://www.linkedin.com/in/livia-popa-23a018183/), we will be working with you through today's tutorial.  We will be using R studio for today's session. 

We will be tackling these objectives:

- *Define* open data and reproducible science
- *Describe* how to navigate important aspects of the R/RStudio user-interface
- *Recall* how to extract public data from a web portal (Cardinal) and import it into a  data software (R/RStudio)
- *Demonstrate* understanding of dataset and statistical software through exploratory data analysis plots and numerical summaries

### Meet Mr. Wuf!

Mr. Wuf works for Mount Mitchell State Park in Burnsville, NC and was recently asked by his boss to write a report summarizing rainfall and temperature data for 2021. This report will be used to help optimize 2022 event planning (e.g., fall color viewing) for park visitors and maintenance scheduling for park staff. Mr. Wuf’s wife, Mrs. Wuf, recently told him about the State Climate Office of North Carolina’s new Cardinal and Station Scout data portals. He agrees with her that it would be a great opportunity to check out these new, free tools. After some preliminary sleuthing around Station Scout, he discovered there was a National Weather Service Cooperative Observer Program (COOP; https://www.weather.gov/rah/coop) station on park property (station # 315923). How did he miss this? Once he downloads these data from Cardinal, Mr. Wuf plans to put the skills he learned in an online R programming course to the test for this real-world, work-related project.\

## API 

### API Introduction 

### nClimgrid Data Set

### Importing Data on nClimGrid 

Text on importing **with an API**.

- This explains how to extract data without a package for certain API's 

```{r}
##library(httr)
#library(jsonlite)
#base_url <- "https://www.ncdc.noaa.gov/cdo-web/api/v2/"
#endpoint <- "datasets"
#token <- "gykUMKPJzpcpQonBrUWjbFqYevOPkhwc"
#full_url <- paste0(base_url, "/",endpoint,"/")
#Raw  <- GET(full_url)

#curl -H "token:gykUMKPJzpcpQonBrUWjbFqYevOPkhwc" #"https://www.ncdc.noaa.gov/cdo-web/api/v2/datasets" $.ajax({ url:https://www.ncdc.noaa.gov/cdo-web/api/v2/datasets, data:{/datasets}, headers:{ token:gykUMKPJzpcpQonBrUWjbFqYevOPkhwc} })
```



## Attempting to pull data from RNOAA  - not super successful rn


```{r}

options(noaakey = "--key goes here --")
# a comment goes here

#list of all stations
ghcnd_stations()

#tibble  of all stations given certain lattitude and logitudes
raliegh_stations<-ghcnd_stations()%>%dplyr::filter(latitude>34 & latitude<36 & longitude>-80 & longitude< -75)

 
raliegh_stations

real_ral<-ghcnd(stationid='GHCND:US1NCAL0013')
real_ral
```



```{r}
Ralz_dat <- ncdc(datasetid='GHCND', stationid='GHCND:US1NCAL0013', datatypeid=c('TAVG','PRCP'), startdate = '2011-01-01', enddate="2011-12-31", add_units = TRUE)

Ralz_dat

#medeo_tidy
```

## Machine Learning 


### Content from beg. lectures for time being 

```{r}

cardinal <- read_csv("cardinal_data.csv", 
     col_types = list(`Average Air Temperature (F)` = col_number(), 
         `Maximum Air Temperature (F)` = col_number(), 
         `Minimum Air Temperature (F)` = col_number(), 
         `Average Experimental Leaf Wetness (mV)` = col_number(), 
         `Total Precipitation (in)` = col_number(), 
         `Average Relative Humidity (%)` = col_number(), 
         `Average Soil Moisture (m3/m3)` = col_number(), 
         `Average Soil Temperature (F)` = col_number(), 
         `Average Solar Radiation (W/m2)` = col_number(), 
         `Average Station Pressure (mb)` = col_number()))

cardinal<-drop_na(cardinal)
str(cardinal)
cardinal$Date<-as.Date(cardinal$Date, tryFormats= c("%m/%d/%y"))
view(cardinal)

#changes col names
colnames(cardinal)=c("date","AvgT","MaxT","MinT","AvgLw","Tprep","AvgHum","AvgSm","AvgSt","AvgSr","AvgStp")



cardinal$IfRain<- (cardinal$Tprep>0)
cardinal$IfRain<-as.factor(as.integer(cardinal$IfRain))


```

### Basic Plotting with Ggplot 


```{r}
ggplot(cardinal,aes(x=date,y=AvgT))+geom_line()+labs(title="Total Daily Rainfall by Date",y="Average Tempurature (F) ", x= "Date")
```

- EDA is how we can motivate future ML models!

- We can use forecasting to extend this trend!


## Testing and training data

- concept of seeing how well a model works 

- *cut to nice images of cross-validation?*

### TIme Series forecasting


- We were thinking of using logistic regression  but may not?

```{r}
#logistic regression
fit1 = glm(IfRain~date+AvgT+AvgLw+AvgSt+AvgSr, data=cardinal, family="binomial")
summary(fit1)

#predict something with logistic regression

```




```{r}

# n climate  grid data

temp_ts <- xts(cardinal$AvgT,cardinal$date)
head(temp_ts)
  
autoplot(temp_ts[1:600])

half_temp <-temp_ts[1:600]

library(forecast)
d.arima <- auto.arima(half_temp)
d.forecast <- forecast(d.arima, level = c(90), h = 200)
autoplot(d.forecast)

```


### PCA to cluster rain variable 


- using cardinal data to obsevre *if* there is clustering 

- used for future models

- helps us describe higher dimensional data with **less**


Three general steps: 

  1. Remove heavily correlated columns! 
    - Min Temp and Max Temp for a certain day will correlate with one another!
    
  2. Center Data

Observe: 

```{r}
library(corrplot)
corrplot(cor(cardinal[,-c(1,12)]))
```

- Tells us to remove all but one temperature variable


```{r}
IfRainVar<- cardinal$IfRain
cardshort <- cardinal%>%select(-c(date,IfRain,Tprep,MinT,MaxT))
cardshort

pca_card<- princomp(scale(cardshort,scale=FALSE),cor = FALSE)


plot(pca_card$scores, pch = 16, col =IfRainVar)
legend("topright",c("No Rain","Rain"),pch=16,col=c("black","red"))

```

- Here we can look at how good PCA does at describing changes in data 

- We see 2 components describes 96% of the data's variation ! (This is very good)

```{r}
summary(pca_card)
```



```{r}
screeplot(pca_card, type = "lines")
```


### How are the original variables related to the principal components?

- Does not print small values, less impactful to correlation 

```{r}
loadings(pca_card)
```


- The loading are simple correlations between the principal components and the original variables (Pearson’s r).

- Values closest to 1 (positive) or -1 (negative) will represent the strongest relationships, with zero being uncorrelated.

We see in PC 1 that there is a high positive correlation between AvgSr. We see the correlation between solar radiation and the component direction is quite high. So by looking at the second component or the y-axis of our previous plot: we see for the most part, Leaf wetness correlated well with the occurance of rain.   


- Another visual to observe the impact of each variable on the principal component!  

- Not super pretty here

```{r}
biplot(pca_card)
```





