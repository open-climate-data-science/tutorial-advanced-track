{"cells":[{"cell_type":"markdown","id":"db4c5abe-fffb-4507-9bac-ae9526ae9503","metadata":{},"source":["# OCDS Advanced Track Tutorial"]},{"cell_type":"markdown","id":"78709032-78cd-4f58-b69e-d9ceded84e26","metadata":{},"source":["## Tutorial Learning Goals\n","\n","Welcome to the Open Climate Data Science Workshop advanced track tutorial. We are  [Nick Gawron](https://www.linkedin.com/in/ngawrondata/) and [Livia Popa](https://www.linkedin.com/in/livia-popa-23a018183/), and we will be working with you through today's tutorial.  We will be using R studio for today's session. \n","\n","We will be tackling these objectives:\n","\n","- *Define* open data and reproducible science\n","- *Recognize* the importance of statistical analysis for climate data analysis using cloud based data\n","- *Apply* techniques to access and explore publicly available climate data from the cloud using exploratory data analysis (i.e. tidyverse)\n","- *Create* statistical model to predicts results for a representative case study"]},{"cell_type":"markdown","id":"eae08236-c5c6-4f29-b81f-02d3bf30164f","metadata":{},"source":["### Meet Suzie!\n","\n","Suzie recently finished a Masters degree in data science and joined the NC State University Cooperative Extension team in her home county, Pender County. Several berry and fruit growers that she knows recently asked her to summarize minimum, mean, and maximum daily temperature and annual precipitation trends over the last couple of decades and estimate what next year’s daily temperature trends will be like. They will use this information to decide what types of berry and fruit varieties they decide to grow in the future. Specifically, certain fruit and berries need a certain number of time below a certain temperature (e.g., 45°F or 7.2°C), called chilling hours, to flower and bear fruit [see refs [1](https://content.ces.ncsu.edu/extension-gardener-handbook/14-small-fruits), [2](https://content.ces.ncsu.edu/extension-gardener-handbook/15-tree-fruit-and-nuts), [3](https://products.climate.ncsu.edu/ag/chill-models/)] \n","\n","Besides the trend analysis (including predicting next year’s temperature range), Suzie decided it would also be interesting to compare Pender County results to the results of another other berry and fruit producing counties in the state that have different chilling hour climate ranges [see ref [4](https://site.extension.uga.edu/climate/2015/01/chill-hours-average-and-actual/)]. She chooses McDowell County in the mountains and heads to the National Centers for Environmental Information (NCEI) nClimGrid website, [here](https://noaa-nclimgrid-daily-pds.s3.amazonaws.com/index.html#EpiNOAA/), for the data she needs. The NCEI data will have county aggregated data for the last two decades at a daily time step. That said, she cannot calculate chilling hours with these data, but she can determine which counties experience harsher winters by determining, the total number of days in the late winter (i.e. January, and February) where minimum, mean, and maximum temperatures are below 45°F (7.2°C).\n","\n","![Suzie](images/suzie.png)"]},{"cell_type":"markdown","id":"bedb5da4","metadata":{},"source":["\n","## Open Data Science\n","\n","### What do you know about open data and reproducible science?\n","\n","Please respond to these questions in the Zoom chat, when prompted:\n","\n","1. Without looking at the section below, how would you define open data?\n","\n","2. How would you define reproducible (and open) science?\n","\n","3. Are you using open science in your work now? If so, how? If not, why not?\n","\n","\n","### What is Open Data and Reproducible Science?\n","\n","Open data documents and shares research data openly for re-use.\n","\n","Open data research aims to transform research by pushing change in the way that research is carried out and disseminated by digital tools. Open data should be:\n","\n","- Publicly available: Open data is freely available on the internet.\n","- Reusable: Proper licensing is essential for research outputs so that users know any limitations on re-use\n","- Transparent: With appropriate metadata to explain how research output was produced and what it contains\n","- You can easily share what you did with your colleagues, collaborators, etc. and it’s easy to make changes and rerun analysis with different settings.\n","\n","We note that reproducible science is when an authors such as Suzie provide all the necessary data and the code to run their analysis again, re-creating the same results.\n","\n","When we combine these idea together - we get the goals for this tutorial.\n","\n","For more information go to this [handbook](https://the-turing-way.netlify.app/reproducible-research/reproducible-research.html) on the subject.\n"]},{"cell_type":"markdown","id":"35359720-e152-4db6-ab46-8dc462282832","metadata":{},"source":["## Importing Data\n","\n","### Cloud Based Data Introduction \n","\n","- Cloud storage is a service model in which data is transmitted and stored on remote storage systems, where it is maintained, managed, backed up and made available to users over a network\n","\n","- Cloud based data is stored in logical pools across storage servers located on premises or in a data center managed by a third party cloud provider such as amazon web services (aws)\n","\n","\n","![](images/cloud_data.jpg)"]},{"cell_type":"markdown","id":"dceb395c-57d8-464b-8dd0-a651f8ca528a","metadata":{},"source":["### nClimGrid Data Set\n","\n","- The NOAA Daily U.S. Climate Gridded Dataset (nClimGrid) consists of four climate variables derived from the station observation networks: maximum temperature, minimum temperature, average temperature and precipitation. \n","\n","- Each file provides daily values in a 5 x 5 kilometers grid for the Continental United States. Data is available from 1950s to the present. \n","\n","- On an annual basis, approximately one year of \"final\" nClimGrid will be submitted to replace the initially supplied \"preliminary\" data for the same time period. \n","\n","- We can find nClimGrid Data hosted on Amazon Web Service (AWS) [here](https://noaa-nclimgrid-daily-pds.s3.amazonaws.com/index.html#EpiNOAA/). There are different versions of the data including orginal 5km x 5km grids, county aggregated, and state aggregated data.\n"]},{"cell_type":"markdown","id":"7c9d80f9","metadata":{},"source":["Let's start by loading the R packages we'll need for this tutorial."]},{"cell_type":"code","execution_count":null,"id":"7e55729a-9c65-4b2f-9640-1fa9a1de8caa","metadata":{},"outputs":[],"source":["# Set up the environment by loading libraries\n","# package for plotting \n","library(ggplot2)\n","# package for cleaning data and logical things \n","library(tidyverse)\n","\n","# time series related packages\n","library(forecast)\n","library(xts)\n","library(TSstudio)\n","\n","# other packages for kmeans\n","library(here)\n","library(tidymodels)\n","library(ClusterR)\n","library(cluster)\n","library(broom)\n","\n","# allows us to read in large amounts of data\n","library(data.table)\n","\n","# plotting in 3d\n","library(plotly)\n","\n","# library \n","library(usmap)\n","\n","# for date manipulation \n","library(lubridate)\n","\n","# for PCA\n","library(corrplot)"]},{"cell_type":"code","execution_count":null,"id":"e303a6db-6624-49e0-8530-bec78e2a4cea","metadata":{},"outputs":[],"source":["# Read in data from AWS nClimGrid 2020-2021\n","# From the decadal section with state labels\n","# Drop missing values\n"]},{"cell_type":"code","execution_count":null,"id":"b2bf2c90-fc00-49da-8b5f-d9bcdbbfa423","metadata":{},"outputs":[],"source":["# Read in nClimGrid 2010-2019\n","# From decadal with state labels\n","# Use the fread command from data.table\n","# Drop missing values\n","\n"]},{"cell_type":"code","execution_count":null,"id":"7a8276cf-3085-47b6-8b2b-2bcc29f211df","metadata":{},"outputs":[],"source":["# If computation time takes a while ... we have   a .Rda file \n"]},{"cell_type":"markdown","id":"2c1ed93c-ecea-4f15-a83f-d1907845372a","metadata":{},"source":["## Functions/Cleaning the Data\n","\n","### Combining Data Sets from Different Decades\n","\n","- We are going to make two datasets `nclim` as well as `nclimk_raw`\n","- `nclim` will be used for time series analyses and `nclimk_raw` will be used for k means clustering"]},{"cell_type":"code","execution_count":null,"id":"74b1e667-65af-4b74-90da-e3c689a84fc8","metadata":{},"outputs":[],"source":["# Lets quickly combine the data sets to get a data set ranging from the above\n","# lets also filter for data record that pertain to North Carolina\n"]},{"cell_type":"code","execution_count":null,"id":"8a96eeee-ad1f-40f6-8af1-7c70439b18a0","metadata":{},"outputs":[],"source":["# We are looking at values from January,March,April,July for all the US here, we will eventually subset to a certain state!\n","# Read in the data with county labels, call it nclimk_raw\n","\n","nclimk_raw <- data.frame()\n","\n","for (Month_val in 1:2){\n","  \n","\n","  \n","}"]},{"cell_type":"markdown","id":"2612757a-5df1-4f0a-980e-80bf13bebe70","metadata":{},"source":["### Functions\n","\n","Using the formula $(C \\times \\frac{9}{5}) + 32=F$, covert the temperature to Fahrenheit. \n"]},{"cell_type":"code","execution_count":null,"id":"0e637885-f8b6-4e95-ac6b-f63c787b323d","metadata":{},"outputs":[],"source":["# create a function for basic syntax \n","# convert data value temperature F to C\n","# Call the function CtoF\n"]},{"cell_type":"markdown","id":"44d9b78b-439a-48af-b2c8-f81297f7e1e5","metadata":{},"source":["We now will need to apply this function to the temperature columns. "]},{"cell_type":"code","execution_count":null,"id":"1d6283e2-187c-4d98-b5d1-a47b2232fef5","metadata":{},"outputs":[],"source":["# apply the function  CtoF with tidyverse functions \n","# to the columns relating to temperature\n","\n","\n","# check the first few rows\n"]},{"cell_type":"markdown","id":"1d836dcb-cdf8-4d71-bb2d-bdd3e3825cd4","metadata":{},"source":["### Cleaning nclim Data"]},{"cell_type":"code","execution_count":null,"id":"6e67cd3a-635a-49e7-b19c-932b1b224d45","metadata":{},"outputs":[],"source":["# Apply Tidyverse piping to easily add degree labels to the end of certain columns for temp\n","# and to add units for precipitation\n","# Inspect \n"]},{"cell_type":"code","execution_count":null,"id":"6d3249f1-1089-4b6d-bfbe-0473642994ba","metadata":{},"outputs":[],"source":["# Keep the weather data columns as well as the date, drop all other columns \n","# Further Create the variable ifRain: a factor to indicate whether it rained on a certain day\n","# Call this final dataset nclimf\n"]},{"cell_type":"markdown","id":"724a4f85-1d7d-473e-8eec-aa9deafc6a8a","metadata":{},"source":["### Cleaning nclimk Data\n","\n","Below we do the same cleaning and data preparation that we did for `nclim`, with the exclusion of the `ifNC` variable. This is because we will be looking at all of the data."]},{"cell_type":"code","execution_count":null,"id":"45850e0e-23f9-4e84-9d12-2175e46f0927","metadata":{},"outputs":[],"source":["# pick a certain state\n","state2consider <- \"NC\" \n","\n","# filter out that state and select relevant columns\n","nclimk_last <- nclimk_raw %>%\n","               filter(state == state2consider) %>%\n","               select(-c(state, month, region_code, year, day)"]},{"cell_type":"code","execution_count":null,"id":"9962f24e","metadata":{},"outputs":[],"source":["# apply function CtoF\n","nclimk_last[4:6] <- nclimk_last[4:6] %>% \n","    lapply(CtoF)"]},{"cell_type":"code","execution_count":null,"id":"c0d3f954","metadata":{},"outputs":[],"source":["# rename temperature colnames \n","colnames(nclimk_last)[4:6] <- colnames(nclimk_last)[4:6] %>%\n","                            tolower()%>%\n","                            paste0(\"_degf\")"]},{"cell_type":"code","execution_count":null,"id":"6df420fc","metadata":{},"outputs":[],"source":["#rename precipitation colnames \n","colnames(nclimk_last)[3] <- colnames(nclimk_last)[3] %>%\n","                          tolower() %>% \n","                          paste0(\"_cm\")"]},{"cell_type":"code","execution_count":null,"id":"f00ea450","metadata":{},"outputs":[],"source":["# set date format of date column\n","nclimk_last$date <- nclimk_last$date %>% \n","    as.Date(\"%m/%e/%Y\")"]},{"cell_type":"code","execution_count":null,"id":"08104356","metadata":{},"outputs":[],"source":["# check the first few rows of the data\n","head(nclimk_last)"]},{"cell_type":"code","execution_count":null,"id":"60518ffa-3c35-4cd2-aaa8-8cdd1c8ab977","metadata":{},"outputs":[],"source":["# check the last few rows of the data\n","tail(nclimk_last)"]},{"cell_type":"markdown","id":"003a865c-a79c-47e1-9ccd-b56b9cad8f9d","metadata":{},"source":["## Explanatory Data Analysis\n","\n","We will inspect the data by creating a couple of graphs, including a scatterplot of temperature vs time and a plot with a simple linear regression."]},{"cell_type":"code","execution_count":null,"id":"0ab0ef12-6385-48cd-afbc-7fcd9ff5661c","metadata":{},"outputs":[],"source":["# scatterplot of avg tempurature vs time, color the points red\n","# add labels\n","# save the plot at plotOTemp object\n"]},{"cell_type":"code","execution_count":null,"id":"41eb961b","metadata":{},"outputs":[],"source":["# show the plot\n"]},{"cell_type":"markdown","id":"1f843233-fe4e-40b4-b836-dfd708c2e49a","metadata":{},"source":["### Statistical Modeling\n","\n","#### Simple Regression \n","\n","We will perform a simple regression see if that model fits the data well.\n"]},{"cell_type":"code","execution_count":null,"id":"70bbd9a7-4f69-44c2-ad68-8969b6a151ac","metadata":{},"outputs":[],"source":["# Using the plotOtemp object create a linear regression for the same data\n","# add linear regression (hint, you can use geom_smooth)\n","# use the classic theme\n"]},{"cell_type":"markdown","id":"31c84da8-f7eb-4155-9c49-2804618e99e0","metadata":{},"source":["## Time Series Analysis\n","\n","- A time series is a collection of observations of well-defined data items obtained through repeated measurements over time\n","\n","- Time series analyze a sequence of data points collected over an interval of times\n","\n","- We will now try to fit a time series model to this data rather than a regression to see if the results improve. We will create a time series forecast with ARIMA (Auto Regressive Integrated Moving Average).\n","\n","- First we begin by creating an `xts` object (a time series project in R).\n","\n","- A useful [cheatsheet](https://www.r-bloggers.com/2017/05/xts-cheat-sheet-time-series-in-r/)"]},{"cell_type":"code","execution_count":null,"id":"7636d4d0-7dd3-4ab8-88af-e15b16a37ab2","metadata":{},"outputs":[],"source":["# with the variables date and average temperature, create a time series object. \n","# save as temp_ts\n"]},{"cell_type":"markdown","id":"eed5386a-fc99-4f1f-8528-7a99ee945b2f","metadata":{},"source":["Now we will split the data into a training and testing set. Majority of the data will be in the training set."]},{"cell_type":"code","execution_count":null,"id":"2c318ccf-0404-4d0f-a553-9b227f6526b3","metadata":{},"outputs":[],"source":["# train/validation split - we want to split the data set into two groups with\n","# 70% of data goes to the training data, split by date\n"]},{"cell_type":"code","execution_count":null,"id":"5db551fe","metadata":{},"outputs":[],"source":["# save the train and test splits based on train_date and the index of temp_ts\n"]},{"cell_type":"markdown","id":"81781f6d-94f0-4d69-bcc7-7e7ec74a63c2","metadata":{},"source":["Next, we plot the time series object we created to take a closer look at the data"]},{"cell_type":"code","execution_count":null,"id":"a46b18eb-b1c9-4ce4-a0da-41883c6adee5","metadata":{},"outputs":[],"source":["# plot the time series object we called temp_ts\n"]},{"cell_type":"markdown","id":"70356f75-3d02-4e6f-bcfc-675a145ec608","metadata":{},"source":["Time to build the time series model!\n","More information on the Autoregressive Integrated Moving Average (ARIMA) model can be found [here](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)"]},{"cell_type":"code","execution_count":null,"id":"12af56ff-df84-4c4e-854f-1e678490b666","metadata":{},"outputs":[],"source":["# build the time series model\n"]},{"cell_type":"markdown","id":"52c66ad6-14a7-4174-b388-368c59561426","metadata":{},"source":["Now we will plot the test data"]},{"cell_type":"code","execution_count":null,"id":"a98d4211-7b1c-40be-b2a8-dd5daaba6c39","metadata":{},"outputs":[],"source":["# plot the test data\n"]},{"cell_type":"markdown","id":"b40d819f-8e68-44e9-b670-27627f66a7e5","metadata":{},"source":["Using the time series model to forecast\n","\n","- Once the model has been created, we will use that to create a forecasting `xts` object\n","\n","- Finally we plot the validation (testing) data and layer the forecast on top for our time series model\n","\n","- The way that this model is interpreted is that the black portion of the graph is the data that was used to predict the blue portion. The forecast shows the average prediction values (blue line) with the associated forecast errors."]},{"cell_type":"code","execution_count":null,"id":"f6a02493-180d-4ea5-a808-6dc631d03291","metadata":{},"outputs":[],"source":["# plot validation data with forecast using autoplot\n"]},{"cell_type":"markdown","id":"4d8be779-d7cb-46e7-b7b4-af8f8cb67401","metadata":{},"source":["## K-Means Clustering Analysis"]},{"cell_type":"markdown","id":"75daa5f7-e63d-415b-85a7-ca19094225ac","metadata":{},"source":["### Clustering Using nClimGrid County Aggregated Data \n","\n","#### Creating Functions\n","\n","- We want to create a function that can do a complicated process in as few steps as possible. \n","\n","- `calculate_cluster` is the name of the function we are going to create to compute k-means clusters among other information. "]},{"cell_type":"code","execution_count":null,"id":"d7940b26-f1f4-4196-84c1-039856d71cf5","metadata":{},"outputs":[],"source":["# Create a function to take in data and a number of k clusters\n","# We want to scale input data, perform k means, and output a dataframe with assigned clusters and silhouette score\n","\n","calculate_cluster <- function(data, k) {\n","  \n","}"]},{"cell_type":"code","execution_count":null,"id":"f6f37830-dd29-4bc1-b8c0-27729a76bf6d","metadata":{},"outputs":[],"source":["# Convert nclimlast to data frame, call it nclimk\n"]},{"cell_type":"code","execution_count":null,"id":"dd53095c","metadata":{},"outputs":[],"source":["# Use plot_ly to create a 3D scatterplot\n","# plot to the data of average tempurature w.r.t date\n"]},{"cell_type":"code","execution_count":null,"id":"6f35bab5-bb3a-4f40-bd38-9cafd512c113","metadata":{},"outputs":[],"source":["# data subset nclimk\n","# we want to only look at the numeric variables\n","# omit any missing values \n","# inspect data\n"]},{"cell_type":"markdown","id":"999ecf84-20b2-45a8-bf84-6638d8c3945a","metadata":{},"source":["### Using Custom Functions to Find the Optimal Number of Clusters\n","\n","Now we want to set up a data frame to determine which runs \n","\n","Below we will use the `map` function, more help can be found from this\n","[link](https://www.rdocumentation.org/packages/purrr/versions/0.2.5/topics/map)\n","\n","This works similar to `lapply` but runs C++ in the background"]},{"cell_type":"code","execution_count":null,"id":"fb671032-f2a4-4c89-96fb-f323d2260911","metadata":{},"outputs":[],"source":["# map cluster calculations function to range of k values\n","# make cluster and silhoutte scores for each k\n","# Inspect data\n","\n","# It is important to set the seed number to make sure \n","# the results are reproducible every time when we rerun the code\n","set.seed(314)\n"]},{"cell_type":"code","execution_count":null,"id":"9e4f2859","metadata":{},"outputs":[],"source":["# look at the first few rows\n"]},{"cell_type":"code","execution_count":null,"id":"75db2f97-72b8-45a3-b7b0-793813854752","metadata":{},"outputs":[],"source":["# check the structure of the data\n"]},{"cell_type":"code","execution_count":null,"id":"ad0e4955-107d-4779-a42c-f595aee5ec79","metadata":{},"outputs":[],"source":["# calculate average silhoutte score (highest for optimal number of k clusters)\n","# for each k value\n","# store these values for each value of k as temp_sil_score_data\n","\n","\n","# look at the first few rows\n"]},{"cell_type":"markdown","id":"cac0419a","metadata":{},"source":["#### Silhouette Scores\n","\n","The hightest silhouette score indicates the optimal number of k clusters.\n","\n","For more on silhouette scores see [this paper](https://www.sciencedirect.com/science/article/pii/0377042787901257)."]},{"cell_type":"code","execution_count":null,"id":"1f196d37-35b4-456a-96ff-b2aae0448f1a","metadata":{},"outputs":[],"source":["# find the maximum sil\n"]},{"cell_type":"code","execution_count":null,"id":"6daa96f7","metadata":{},"outputs":[],"source":["\n","# save optimal k\n"]},{"cell_type":"markdown","id":"731f5e47-dcf7-4bae-8def-94b42bd08553","metadata":{},"source":["Note: We could have also used a visual inspection"]},{"cell_type":"code","execution_count":null,"id":"1212cf76-cde9-4ecb-8913-7a01dcd378c2","metadata":{},"outputs":[],"source":["# plot the data from temp_sil_score_data and visually inspect to determine the k\n","# with the largest sil score\n","# Add a marker point to visually show the max value!\n","temp_sil_score_data %>%\n","  ggplot(...) +\n","  ...\n","  \n","  labs(x = \"Value of k\", y = \"Mean of Silhouette Score\", title = \"Silhouette Score for Value of k\") + ...\n","\n","# save plot as image \n"]},{"cell_type":"code","execution_count":null,"id":"fdf9b60d-4a1e-48c6-9dc6-85958747a5b0","metadata":{},"outputs":[],"source":["# save optimal k cluster data as nclimk_optimal_cluster\n"]},{"cell_type":"code","execution_count":null,"id":"c3dda611-8ea0-4b5b-a99c-c94779614ecf","metadata":{},"outputs":[],"source":["# Combine nclim_optimal_cluster with appropriate county data as well as date\n","# Update column names\n","# Call this ClusterWCounty\n"]},{"cell_type":"markdown","id":"04d3a92d-d6bf-4b1a-96ab-a1cd7d333bc4","metadata":{},"source":["We have a function to compute the mode of a data set. i.e. This will help us compute the determine the cluster every county most often belongs too. "]},{"cell_type":"code","execution_count":null,"id":"f5cd1f4a-70df-4903-9601-ff93d517a563","metadata":{},"outputs":[],"source":["# function for the mode\n","getmode <- function(v) {\n","   uniqv <- unique(v)\n","   uniqv[which.max(tabulate(match(v, uniqv)))]\n","}"]},{"cell_type":"code","execution_count":null,"id":"6e693cf9-2a14-40be-8f83-6255cab99ce6","metadata":{},"outputs":[],"source":["# Create a common cluster vector for the new data frame called countyplot\n","# Change the name of the county to the appropriate fips code\n","# use the mode function to compute the most common cluster per county\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# overwrite this tibble as a dataframe and look at the first few rows\n"]},{"cell_type":"code","execution_count":null,"id":"d6664549-1253-4ee2-ba19-6362691a1d58","metadata":{},"outputs":[],"source":["# Using plot_usmap to plot North Carolina counties with their appropriate clusters\n","plot_usmap( data = countyPlot, \n","            values = \"Common_Cluster\", \"counties\", \n","            include = c(state2consider), \n","            color=\"black\")+\n","            labs(title=paste0(state2consider,\" Cluster Mapping\"))+  \n","            #can comment out the below line if we use the mode method. \n","            #scale_fill_continuous(low = \"#FFCC00\", high = \"#CC0000\", name=\"Common_Cluster\", label=scales::comma)+\n","            theme(legend.position=\"right\") "]},{"cell_type":"markdown","id":"983edcc5-267b-4acb-aed3-8394dae5e273","metadata":{},"source":["Let us examine this topographical map from [geology.com]()\n","\n","![NC Map](images/NCelv.PNG)"]},{"cell_type":"code","execution_count":null,"id":"b8cb0659-7e67-4b04-962c-425e0a4d7925","metadata":{},"outputs":[],"source":["# create a 3d plot for all of our clusters, same as before but now apply cluster coloring from ClusterWCounty\n"]},{"cell_type":"markdown","id":"abbdc12f-b052-4b3c-ad5d-9452ff28580a","metadata":{},"source":["Now let us look at the data cluster assignment density for Suzie's analysis:  Pender County.  "]},{"cell_type":"code","execution_count":null,"id":"880bee09-4aae-49d7-a957-6a835242310d","metadata":{},"outputs":[],"source":["## Filter Data by Pender County\n","\n","\n","# look at the first few rows\n"]},{"cell_type":"code","execution_count":null,"id":"1f2c54c1-03e5-4c9d-8b9f-b090d7ab2abc","metadata":{},"outputs":[],"source":["# plot outputs as the density of doy colored by cluster id\n"]},{"cell_type":"markdown","id":"e665bedf-fe2f-45f0-a781-45032026dc3f","metadata":{},"source":["We see a major influence in Pender County "]},{"cell_type":"code","execution_count":null,"id":"7a02ca6e-151f-4659-8354-2b44a0ae17d3","metadata":{},"outputs":[],"source":["# Observe Pender County Weather Data in Clusters \n","\n","\n","\n","# Further we can compare the averages acorss MCdoweel and Pender\n"]},{"cell_type":"markdown","id":"4bd5281c-fa8f-4ccf-b26e-0c9881c7af5e","metadata":{},"source":["#### Some observations\n","\n","- We can observe stark differences in the tempurature and rain across these counties \n","  - Evident why they are in differing clusters\n","- Knowing these differences will better inform Suzie's berry growing friends!\n","\n","What else do you notice that Suzie might want to pay attention to?"]},{"cell_type":"markdown","id":"74344232-256a-485e-a081-6e9dac17baf7","metadata":{},"source":["## Bonus (for time) Content!\n","\n","### PCA with ECOnet Data from the Cardinal Data Portal\n","\n","For more on the Cardinal data portal see the beginner tutorial from this workshop."]},{"cell_type":"code","execution_count":null,"id":"ad3e053a-a104-4b97-af65-0321400fe87d","metadata":{},"outputs":[],"source":["cardinal <- read_csv(\"cardinal_data.csv\", \n","     col_types = list(`Average Air Temperature (F)` = col_number(), \n","         `Maximum Air Temperature (F)` = col_number(), \n","         `Minimum Air Temperature (F)` = col_number(), \n","         `Average Experimental Leaf Wetness (mV)` = col_number(), \n","         `Total Precipitation (in)` = col_number(), \n","         `Average Relative Humidity (%)` = col_number(), \n","         `Average Soil Moisture (m3/m3)` = col_number(), \n","         `Average Soil Temperature (F)` = col_number(), \n","         `Average Solar Radiation (W/m2)` = col_number(), \n","         `Average Station Pressure (mb)` = col_number()))\n","\n","cardinal <- drop_na(cardinal)\n","str(cardinal)\n","cardinal$Date <- as.Date(cardinal$Date, tryFormats = c(\"%m/%d/%y\"))\n","view(cardinal)\n","\n","#changes col names\n","colnames(cardinal) = c(\"date\",\"AvgT\",\"MaxT\",\"MinT\",\"AvgLw\",\"Tprep\",\"AvgHum\",\"AvgSm\",\"AvgSt\",\"AvgSr\",\"AvgStp\")\n","\n","cardinal$IfRain <- (cardinal$Tprep>0)\n","cardinal$IfRain <- as.factor(as.integer(cardinal$IfRain))"]},{"cell_type":"markdown","id":"f44e7aaa-1615-44cf-902e-d5cfd55f7c16","metadata":{},"source":["### Basic Plotting with GGplot "]},{"cell_type":"code","execution_count":null,"id":"0efc29ff-416a-40a2-b328-fd6ecdca3820","metadata":{},"outputs":[],"source":["ggplot(data = cardinal, mapping = aes(x = date, y = AvgT)) +\n","    geom_line() + \n","    labs(title=\"Total Daily Rainfall by Date\",\n","         y=\"Average Tempurature (F) \", \n","         x= \"Date\")"]},{"cell_type":"markdown","id":"3cffd9bd-eebb-4893-b85c-60d07186ce72","metadata":{},"source":["- EDA is how we can motivate future ML models!\n","\n","- We can use forecasting to extend this trend!"]},{"cell_type":"markdown","id":"57c5460c-7076-474f-9910-20aa0f46bf04","metadata":{},"source":["### PCA to cluster rain variable \n","\n","\n","- using cardinal data to observe *if* there is clustering \n","\n","- used for future models\n","\n","- helps us describe higher dimensional data with **less**\n","\n","\n","Three general steps: \n","\n","  1. Remove heavily correlated columns! \n","    - Min Temp and Max Temp for a certain day will correlate with one another!\n","    \n","  2. Center Data\n","\n","Observe: "]},{"cell_type":"code","execution_count":null,"id":"77a37b03-33aa-4d9f-b6a5-cfe465735d96","metadata":{},"outputs":[],"source":["library(corrplot)\n","corrplot(cor(cardinal[,-c(1, 12)]))"]},{"cell_type":"markdown","id":"e0ddf686-ba0c-4256-b3e8-9e72e3ef7be7","metadata":{},"source":["- Tells us to remove all but one temperature variable"]},{"cell_type":"code","execution_count":null,"id":"e96ca3d9-6001-4967-a373-0ad3adb30aee","metadata":{},"outputs":[],"source":["IfRainVar <- cardinal$IfRain\n","cardshort <- cardinal %>% \n","    select(-c(date, IfRain, Tprep, MinT, MaxT))\n","head(cardshort)"]},{"cell_type":"markdown","id":"53126de5-7a32-4cd3-aac7-60f256bce6ab","metadata":{},"source":["- We will now actually conduct PCA on our data\n"]},{"cell_type":"code","execution_count":null,"id":"eaddbd93-8528-49e8-b481-44ea3879d41b","metadata":{},"outputs":[],"source":["pca_card <- princomp(scale(cardshort, scale = FALSE), cor = FALSE)"]},{"cell_type":"code","execution_count":null,"id":"adde51ad-c992-4a87-b0f8-590bafd04de9","metadata":{},"outputs":[],"source":["plot(pca_card$scores, pch = 16, col = IfRainVar)\n","\n","legend(\"topright\", c(\"No Rain\", \"Rain\"), pch = 16, col = c(\"black\", \"red\"))"]},{"cell_type":"markdown","id":"be5de50b-08b4-47c9-8a22-ef6226e7a07f","metadata":{},"source":["- Here we can look at how good PCA does at describing changes in data \n","\n","- We see 2 components describes 96% of the data's variation ! (This is very good)"]},{"cell_type":"code","execution_count":null,"id":"17d5d379-d59a-4a6e-aaed-fa8ce869e583","metadata":{},"outputs":[],"source":["summary(pca_card)"]},{"cell_type":"code","execution_count":null,"id":"461984ca-1b04-40b9-af7e-631e5480e4f3","metadata":{},"outputs":[],"source":["screeplot(pca_card, type = \"lines\")"]},{"cell_type":"markdown","id":"4811a4fc-ce37-4ce9-ba9f-b2b030aa027c","metadata":{},"source":["### How are the original variables related to the principal components?\n","\n","- Does not print small values, less impactful to correlation "]},{"cell_type":"code","execution_count":null,"id":"aba4af61-531c-4702-8437-4370a1576398","metadata":{},"outputs":[],"source":["loadings(pca_card)"]},{"cell_type":"markdown","id":"22fb3055-1dc8-4210-acd2-46caae1f64d2","metadata":{},"source":["- The loading are simple correlations between the principal components and the original variables (Pearson’s r).\n","\n","- Values closest to 1 (positive) or -1 (negative) will represent the strongest relationships, with zero being uncorrelated.\n","\n","We see in PC 1 that there is a high positive correlation between AvgSr. We see the correlation between solar radiation and the component direction is quite high. So by looking at the second component or the y-axis of our previous plot: we see for the most part, Leaf wetness correlated well with the occurance of rain.   \n","\n","\n","- Another visual to observe the impact of each variable on the principal component!  \n","\n","- Not super pretty here"]},{"cell_type":"code","execution_count":null,"id":"2d34178e-8874-4764-be15-57e709975676","metadata":{},"outputs":[],"source":["biplot(pca_card)"]},{"cell_type":"markdown","id":"5e8dd3d2-0580-4fd3-89eb-489b1980b8d7","metadata":{},"source":["## Closing & Resources\n","\n","- *Define* open data and reproducible science\n","- *Recognize* the importance of statistical analysis for climate data analysis using cloud based data\n","- *Apply* techniques to access and explore publicly available climate data from the cloud using exploratory data analysis\n","- *Create* a statistical model to predicts results for a representative case study\n","\n","### Question for the Zoom Chat\n","\n","What else can Suzie include or add to for her analysis?\n","\n","### Resouces\n","\n","- For a A Hydrologists Guide to Open Science, from the HESS Journal, click [here](https://hess.copernicus.org/articles/26/647/2022/). \n","- Please see a full list of resources on the workshop website [Resources page](https://open-climate-data-science.github.io/resources/)\n","\n","### Survey \n","\n","Thank you for attending the beginner track tutorial at the Open Climate Data Science Workshop. Please complete the voluntary feedback survey, linked [here](https://docs.google.com/forms/d/e/1FAIpQLScx3Vri5m2qrSYQGodt-bsFyImg-8P57OKCuuH-UadMVinmXA/viewform).\n","\n","The main purposes of this survey is to: \n","\n","1. determine we met specified teaching goals and\n","2. improve teaching materials for subsequent tutorial sessions."]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"codemirror_mode":"r","file_extension":".r","mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"4.1.2"}},"nbformat":4,"nbformat_minor":5}
